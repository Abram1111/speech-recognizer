{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If true, the WAV files will be read and their features will be saved in the CSV files\n",
    "# As this is the most time consuming task, only enable it if you don't have the CSV files yet\n",
    "CREATE_CSV_FILES = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the names of the CSV files\n",
    "TRAIN_CSV_FILE      = \"train.csv\"\n",
    "TEST_CSV_FILE       = \"test.csv\"\n",
    "\n",
    "# TRAIN_CSV_FILE_CLose =\"train_close.csv\"\n",
    "# TRAIN_CSV_FILE_Open  =\"train_open.csv\"\n",
    "# TEST_CSV_FILE_CLose =\"test_close.csv\"\n",
    "# TEST_CSV_FILE_Open  =\"test_open.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import librosa\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def extractWavFeatures(soundFilesFolder, csvFileName,genre):\n",
    "    print(\"The features of the files in the folder \"+soundFilesFolder+\" will be saved to \"+csvFileName)\n",
    "    # Header\n",
    "    header = 'state filename chroma_stft rmse spectral_centroid spectral_bandwidth rolloff zero_crossing_rate'\n",
    "    for i in range(1, 21):\n",
    "        header += f' mfcc{i}'\n",
    "    header += ' label'\n",
    "    header = header.split()\n",
    "    print('CSV Header: ', header)\n",
    "    # preaparing csv file\n",
    "    file = open(csvFileName, 'w', newline='')\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # looping through records\n",
    "    for filename in os.listdir(soundFilesFolder):\n",
    "        number  = f'{soundFilesFolder}/{filename}'\n",
    "        state   =genre\n",
    "        # reading records\n",
    "        y, sr = librosa.load(number)\n",
    "        # remove leading and trailing silence\n",
    "        y, index    = librosa.effects.trim(y)\n",
    "\n",
    "        state       = genre\n",
    "        chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        rmse        = librosa.feature.rms(y=y)\n",
    "        spec_cent   = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        spec_bw     = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "        rolloff     = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        zcr         = librosa.feature.zero_crossing_rate(y)\n",
    "        mfcc        = librosa.feature.mfcc(y=y, sr=sr)\n",
    "        to_append   = f'{state} {filename} {np.mean(chroma_stft)} {np.mean(rmse)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}'\n",
    "        for e in mfcc:\n",
    "            to_append += f' {np.mean(e)}'\n",
    "        writer.writerow(to_append.split())\n",
    "    file.close()\n",
    "    print(\"End of extractWavFeatures\")\n",
    "\n",
    "if (CREATE_CSV_FILES == True):\n",
    "    # extractWavFeatures(\"train/open\" , TRAIN_CSV_FILE_Open ,1)\n",
    "    # extractWavFeatures(\"train/close\" , TRAIN_CSV_FILE_CLose,0)\n",
    "\n",
    "    # extractWavFeatures(\"test/open\"  , TEST_CSV_FILE_Open ,1)\n",
    "    # extractWavFeatures(\"test/close\" , TEST_CSV_FILE_CLose,0)\n",
    "\n",
    "    # extractWavFeatures(\"../data/recordings/moreSpeakersTrain\", MORE_TRAIN_CSV_FILE)\n",
    "    # extractWavFeatures(\"../data/recordings/moreSpeakersTest\", MORE_TEST_CSV_FILE)\n",
    "    print(\"CSV files are created\")\n",
    "else:\n",
    "    print(\"CSV files creation is skipped\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Using Entropy:---------------------\n",
      "[[ 8.65143100e-03  1.47756177e+03  1.90946123e+03 ... -2.64368129e+00\n",
      "  -7.93565798e+00 -4.12586117e+00]\n",
      " [ 1.48427427e-01  1.77548039e+03  1.82374153e+03 ... -1.14715576e+01\n",
      "  -4.17657757e+00 -5.88370132e+00]\n",
      " [ 7.38329500e-03  1.46013622e+03  1.89309178e+03 ...  2.16274762e+00\n",
      "  -1.79716659e+00 -1.10321054e+01]\n",
      " ...\n",
      " [ 2.21242870e-02  2.65793746e+03  2.73963809e+03 ... -1.82252884e+00\n",
      "  -5.81420898e+00 -2.80799389e+00]\n",
      " [ 8.12090900e-03  1.37846442e+03  1.81837766e+03 ... -2.79363632e+00\n",
      "  -5.17642116e+00 -6.52206993e+00]\n",
      " [ 7.89907100e-03  1.68077813e+03  1.94560515e+03 ...  5.99143696e+00\n",
      "  -6.02419472e+00 -1.05705471e+01]]\n",
      "[[0.013147185556590557, 2849.6503584460247, 2326.4879458307123, 5264.329833984375, 0.16525472005208333, -549.3910522460938, 51.08045196533203, 17.5028076171875, 2.9545342922210693, -3.0804476737976074, 3.726375102996826, -21.580549240112305, -2.282191514968872, -17.786502838134766, -9.046612739562988, -12.022527694702148, -5.1477742195129395, -9.306376457214355, 4.342337608337402, -4.887171268463135, -6.653088092803955, 0.5347170233726501, -0.7631245851516724, -2.180628538131714, 1.8753852844238281]]\n",
      "Close\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Importing the required packages\n",
    "import pickle\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def prepare_testing(to_test):\n",
    "    features=[]\n",
    "    # reading records\n",
    "    y, sr = librosa.load(to_test)\n",
    "    # remove leading and trailing silence\n",
    "    y, index    = librosa.effects.trim(y)\n",
    "\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    rmse        = librosa.feature.rms(y=y)\n",
    "    spec_cent   = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spec_bw     = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    rolloff     = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    zcr         = librosa.feature.zero_crossing_rate(y)\n",
    "    mfcc        = librosa.feature.mfcc(y=y, sr=sr)\n",
    "    to_append   = f' {np.mean(rmse)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}'\n",
    "    for e in mfcc:\n",
    "        to_append += f' {np.mean(e)}'\n",
    "    \n",
    "    features.append(to_append.split())\n",
    "\n",
    "    \n",
    "    for index in range(0,len(features[0])):\n",
    "        features[0][index]=float(features[0][index])\n",
    "\n",
    "    return features\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "# Function importing Dataset\n",
    "def importdata():\n",
    "    data = pd.read_csv('train.csv')\n",
    "    data = data.drop(['filename'],axis=1)\n",
    "    data = data.drop(['label'],axis=1)\n",
    "    data = data.drop(['chroma_stft'],axis=1)\n",
    "\n",
    "    # # Printing the dataswet shape\n",
    "    # print (\"Dataset Length: \", len(data))\n",
    "    # print (\"Dataset Shape: \", data.shape)\n",
    "      \n",
    "    # # Printing the dataset obseravtions\n",
    "    # print (\"Dataset: \",data.head())\n",
    "    return data\n",
    "  \n",
    "# Function to split the dataset\n",
    "def splitdataset(balance_data):\n",
    "  \n",
    "    # Separating the target variable\n",
    "    X = balance_data.values[:, 1:]\n",
    "    Y = balance_data.values[:, 0]\n",
    "    # Splitting the dataset into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.3, random_state = 100)\n",
    "      \n",
    "    return X, Y, X_train, X_test, y_train, y_test\n",
    "      \n",
    "# Function to perform training with giniIndex.\n",
    "def train_using_gini(X_train, X_test, y_train):\n",
    "  \n",
    "    # Creating the classifier object\n",
    "    clf_gini = DecisionTreeClassifier(criterion = \"gini\",random_state = 100,max_depth=3, min_samples_leaf=5)\n",
    "    # Performing training\n",
    "    clf_gini.fit(X_train, y_train)\n",
    "    return clf_gini\n",
    "      \n",
    "# Function to perform training with entropy.\n",
    "def tarin_using_entropy(X_train, X_test, y_train):\n",
    "\n",
    "    # Decision tree with entropy\n",
    "    clf_entropy = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100,max_depth = 3, min_samples_leaf = 5)\n",
    "    # Performing training\n",
    "    clf_entropy.fit(X_train, y_train)\n",
    "    return clf_entropy\n",
    "  \n",
    "  \n",
    "# Function to make predictions\n",
    "def prediction(X_test, clf_object):\n",
    "  \n",
    "    # Predicton on test with giniIndex\n",
    "    # print('x_test')\n",
    "    # print(X_test[0])\n",
    "    y_pred = clf_object.predict(X_test)\n",
    "    # print(\"Predicted values:\")\n",
    "    # print(y_pred)\n",
    "    return y_pred\n",
    "      \n",
    "# Function to calculate accuracy\n",
    "def cal_accuracy(y_test, y_pred):\n",
    "      \n",
    "    print(\"Confusion Matrix: \",confusion_matrix(y_test, y_pred))\n",
    "      \n",
    "    print (\"Accuracy : \",accuracy_score(y_test,y_pred)*100)\n",
    "      \n",
    "    print(\"Report : \",classification_report(y_test, y_pred))\n",
    "  \n",
    "# Driver code\n",
    "def main():\n",
    "\n",
    "    # to_predict=[[],[]]\n",
    "    to_predict=prepare_testing('k_close_2.wav')\n",
    "    \n",
    "    # Building Phase\n",
    "    data = importdata()\n",
    "\n",
    "    X, Y, X_train, X_test, y_train, y_test = splitdataset(data)\n",
    "    clf_gini    = train_using_gini(X_train, X_test, y_train)\n",
    "    clf_entropy = tarin_using_entropy(X_train, X_test, y_train)\n",
    "      \n",
    "    # # Operational \n",
    "    # print(\"Results Using Gini Index:-----------------\")\n",
    "      \n",
    "    # # Prediction using gini\n",
    "    # y_pred_gini = prediction(X_test, clf_gini)\n",
    "    # cal_accuracy(y_test, y_pred_gini)\n",
    "      \n",
    "    print(\"Results Using Entropy:---------------------\")\n",
    "    # Prediction using entropy\n",
    "    print(X_test)\n",
    "    print(to_predict)\n",
    "    y_pred_entropy = prediction(to_predict, clf_entropy)\n",
    "    if y_pred_entropy==0:\n",
    "        print('Close')\n",
    "    elif y_pred_entropy==1:\n",
    "        print('Open')\n",
    "\n",
    "    # cal_accuracy(y_test, y_pred_entropy)\n",
    "    model ='model.pkl'\n",
    "    pickle.dump(clf_entropy,open(model,'wb'))\n",
    "      \n",
    "      \n",
    "# Calling main function\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "80675acf6ae80958eb411ca205f95cad4e647ae651ea56317ec1891bd9e4b0bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
